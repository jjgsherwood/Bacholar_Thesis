{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import distributions\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\"../data/\", train=True, download=False, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(\"../data/\", train=False, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-749334e64a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "IM_SIZE = 28 * 28\n",
    "IM_SHAPE = (28, 28)\n",
    "CPU_training = False\n",
    "BATCH = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "use_cuda =  torch.cuda.is_available() and not CPU_training\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=BATCH, shuffle=True, num_workers=7, pin_memory=use_cuda)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=BATCH, shuffle=False, num_workers=7, pin_memory=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nets(size): \n",
    "    return nn.Sequential(nn.Linear(size, 128),\n",
    "                         nn.ReLU(True), \n",
    "                         nn.Linear(128, size),\n",
    "                         nn.Sigmoid())\n",
    "\n",
    "def nett(size):\n",
    "    return nn.Sequential(nn.Linear(size, 128),\n",
    "                         nn.ReLU(True), \n",
    "                         nn.Linear(128, size))\n",
    "\n",
    "class CouplingLayer(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size // 2 \n",
    "        self.translate = nett(self.size)\n",
    "        self.scale = nets(self.size)\n",
    "        \n",
    "    def forward(self, args):\n",
    "        x, log_det_J = args\n",
    "        x1, x2 = torch.split(x, self.size, 1)\n",
    "        s = self.scale(x1)\n",
    "        t = self.translate(x1)\n",
    "        y2 = x2 * torch.exp(s) + t\n",
    "        y = torch.cat((y2, x1), 1)\n",
    "        log_det_J += s.sum(dim=1)\n",
    "        return y, log_det_J\n",
    "\n",
    "    def inverse(self, y):\n",
    "        y1, y2 = torch.split(y, self.size, 1)      \n",
    "        s = self.scale(y2)\n",
    "        t = self.translate(y2)\n",
    "        x1 = (y1 - t) * torch.exp(-s)\n",
    "        return torch.cat((y2, x1), 1) \n",
    "    \n",
    "class CouplingLayer_nice(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size // 2 \n",
    "        self.translate = nett(self.size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1, x2 = torch.split(x, self.size, 1)       \n",
    "        t = self.translate(x1)\n",
    "        y2 = x2 + t\n",
    "        return torch.cat((y2, x1), 1)\n",
    "\n",
    "    def inverse(self, y):\n",
    "        y1, y2 = torch.split(y, self.size, 1)         \n",
    "        t = self.translate(y2)\n",
    "        x1 = y1 - t\n",
    "        return torch.cat((y2, x1), 1)\n",
    "    \n",
    "class HH(nn.Module):\n",
    "    def __init__(self, size, num_vectors=None):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.num_vectors = num_vectors or 2 * (self.size // 2 + 1)\n",
    "        self.vectors = nn.Parameter(torch.Tensor(self.num_vectors, self.size, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.vectors, -1, 1)\n",
    "        self.vectors.data.copy_(self.vectors / self.vectors.norm(dim=1, keepdim=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.bmm_naive_cascade()\n",
    "        return x @ Q.t()\n",
    "\n",
    "    def inverse(self, x):\n",
    "        Q = self.bmm_naive_cascade()\n",
    "        return x @ Q\n",
    "\n",
    "    # Householder transformation\n",
    "    def _get_bmm_householder_matrices(self):\n",
    "        N, S, _ = self.vectors.size()\n",
    "\n",
    "        outer = torch.bmm(self.vectors, self.vectors.transpose(1, 2))\n",
    "        inner = torch.bmm(self.vectors.transpose(1, 2), self.vectors)\n",
    "        I = torch.eye(S, device=self.vectors.device).expand(N, -1, -1)\n",
    "        hh_matrices = I - 2 * outer / (inner + 1e-16)\n",
    "        return hh_matrices\n",
    "\n",
    "    @staticmethod\n",
    "    def _reduce_mm(matrices):\n",
    "        Q = matrices[0]\n",
    "        for M in matrices[1:]:\n",
    "            Q = torch.mm(Q, M)\n",
    "        return Q\n",
    "\n",
    "\n",
    "    def bmm_naive_cascade(self):\n",
    "        \"\"\"\n",
    "        Args:   \n",
    "            vectors: [NumVectors, Size, 1]\n",
    "        Output:\n",
    "            Q: [Size, Size]\n",
    "        \"\"\"\n",
    "        matrices = self._get_bmm_householder_matrices()\n",
    "        return HH._reduce_mm(matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = CouplingLayer_nice(784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.prior = distributions.MultivariateNormal(torch.zeros(size, device=device), torch.eye(size, device=device))\n",
    "        self.encoder = nn.Sequential(CouplingLayer(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size),                                   \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        log_det_J = x.new_zeros(x.shape[0])\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, CouplingLayer):\n",
    "                x, log_det_J = layer((x, log_det_J))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x, log_det_J\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        for layer in reversed(self.encoder):\n",
    "            y = layer.inverse(y) \n",
    "        return y\n",
    "    \n",
    "    def sample(self, batchSize=1):\n",
    "        y = self.prior.sample((batchSize,))\n",
    "        return self.inverse(y)\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        y, logp = self(x)\n",
    "        return self.prior.log_prob(y) - logp\n",
    "    \n",
    "class Nice(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.prior = distributions.MultivariateNormal(torch.zeros(size, device=device), torch.eye(size, device=device))\n",
    "        self.encoder = nn.Sequential(\n",
    "                                     CouplingLayer_nice(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)   \n",
    "    \n",
    "    def inverse(self, y):\n",
    "        for layer in reversed(self.encoder):\n",
    "            y = layer.inverse(y) \n",
    "        return y\n",
    "    \n",
    "    def sample(self, batchSize=1):\n",
    "        y = self.prior.sample((batchSize,))\n",
    "        return self.inverse(y)\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        y = self(x)\n",
    "        return self.prior.log_prob(y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network):\n",
    "    net = network(IM_SIZE).to(device).train()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for X, _ in trainset:\n",
    "            X = X.view(-1, IM_SIZE).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = -net.log_prob(X).mean()\n",
    "            \n",
    "#             plt.imshow(net.inverse(net(X))[0].cpu().detach().view(IM_SHAPE))\n",
    "#             plt.show()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "        print(epoch, loss)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(722.8506, device='cuda:0', grad_fn=<NegBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7b20189a710c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet_RealNVP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-f25700a3589b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(network)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m#             break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Nice(IM_SIZE).to(device).train()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for X, _ in trainset:\n",
    "#             plt.imshow(X[0,0])\n",
    "#             plt.show()\n",
    "        X = X.view(-1, IM_SIZE).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = -net.log_prob(X).mean()\n",
    "\n",
    "#             plt.imshow(net.inverse(net(X))[0].cpu().detach().view(IM_SHAPE))\n",
    "#             plt.show()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "#             break\n",
    "#         break\n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Nice"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net_RealNVP.eval()\n",
    "for X, _ in testset:\n",
    "    X = X[:10].view(-1, IM_SIZE).to(device)\n",
    "    y = net_RealNVP(X)\n",
    "    x = net_RealNVP.inverse(y)\n",
    "    break\n",
    "    \n",
    "for i, im in enumerate(x):  \n",
    "    plt.imshow(X[i].cpu().view(IM_SHAPE).detach().numpy())\n",
    "    plt.show()    \n",
    "    plt.imshow(im.cpu().view(IM_SHAPE).detach().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net):\n",
    "    net.eval()\n",
    "    comp_data = np.empty((10000, 784))\n",
    "    comp_label = np.empty((10000,))\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testset):\n",
    "            X = X.view(-1, IM_SIZE).to(device)\n",
    "            latent = net(X)\n",
    "            output = net.inverse(latent)\n",
    "            comp_data[BATCH * i: BATCH * (i + 1)] = latent.cpu()\n",
    "            comp_label[BATCH * i: BATCH * (i + 1)] = y\n",
    "\n",
    "            loss += F.mse_loss(output, X)\n",
    "\n",
    "    print(loss / (i + 1))\n",
    "    if use_cuda:\n",
    "        X = X.cpu()\n",
    "        output = output.cpu()\n",
    "        \n",
    "    plt.subplot(141)\n",
    "    plt.imshow(X[0].view(IM_SHAPE))\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(comp_data[0].reshape(IM_SHAPE))\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(output[0].view(IM_SHAPE))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comp_data, comp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(net_RealNVP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_RealNVP.eval()\n",
    "X = net_RealNVP.sample().cpu()\n",
    "plt.imshow(X[0].view(IM_SHAPE).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_RealNVP.eval()\n",
    "SAMPLES = 10\n",
    "X = net_RealNVP.sample(SAMPLES).cpu()\n",
    "for i in range(SAMPLES):\n",
    "    plt.imshow(X[i].view(IM_SHAPE).detach().numpy())\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
