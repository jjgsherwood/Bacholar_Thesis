{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import distributions\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"../data/\", train=True, download=True, transform=transforms.ToTensor())\n",
    "test = datasets.MNIST(\"../data/\", train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_SIZE = 28 * 28\n",
    "IM_SHAPE = (28, 28)\n",
    "CPU_training = False\n",
    "BATCH = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "use_cuda =  torch.cuda.is_available() and not CPU_training\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=BATCH, shuffle=True, num_workers=7, pin_memory=use_cuda)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=BATCH, shuffle=False, num_workers=7, pin_memory=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nets(size): \n",
    "    return nn.Sequential(nn.Linear(size, 128),\n",
    "                         nn.ReLU(True), \n",
    "                         nn.Linear(128, size),\n",
    "                         nn.Sigmoid())\n",
    "\n",
    "def nett(size):\n",
    "    return nn.Sequential(nn.Linear(size, 128),\n",
    "                         nn.ReLU(True), \n",
    "                         nn.Linear(128, size))\n",
    "\n",
    "class CouplingLayer(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size // 2 \n",
    "        self.translate = nett(self.size)\n",
    "        self.scale = nets(self.size)\n",
    "        \n",
    "    def forward(self, args):\n",
    "        x, log_det_J = args\n",
    "        x1, x2 = torch.split(x, self.size, 1)\n",
    "        s = self.scale(x1)\n",
    "        t = self.translate(x1)\n",
    "        y2 = x2 * torch.exp(s) + t\n",
    "        y = torch.cat((y2, x1), 1)\n",
    "        log_det_J += s.sum(dim=1)\n",
    "        return y, log_det_J\n",
    "\n",
    "    def inverse(self, y):\n",
    "        y1, y2 = torch.split(y, self.size, 1)      \n",
    "        s = self.scale(y2)\n",
    "        t = self.translate(y2)\n",
    "        x1 = (y1 - t) * torch.exp(-s)\n",
    "        return torch.cat((y2, x1), 1) \n",
    "    \n",
    "class CouplingLayer_nice(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size // 2 \n",
    "        self.translate = nett(self.size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1, x2 = torch.split(x, self.size, 1)       \n",
    "        t = self.translate(x1)\n",
    "        y2 = x2 + t\n",
    "        return torch.cat((y2, x1), 1)\n",
    "\n",
    "    def inverse(self, y):\n",
    "        y1, y2 = torch.split(y, self.size, 1)         \n",
    "        t = self.translate(y2)\n",
    "        x1 = y1 - t\n",
    "        return torch.cat((y2, x1), 1)\n",
    "    \n",
    "class HH(nn.Module):\n",
    "    def __init__(self, size, num_vectors=None):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.num_vectors = num_vectors or 2 * (self.size // 2 + 1)\n",
    "        self.vectors = nn.Parameter(torch.Tensor(self.num_vectors, self.size, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.vectors, -1, 1)\n",
    "        self.vectors.data.copy_(self.vectors / self.vectors.norm(dim=1, keepdim=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.bmm_naive_cascade()\n",
    "        return x @ Q.t()\n",
    "\n",
    "    def inverse(self, x):\n",
    "        Q = self.bmm_naive_cascade()\n",
    "        return x @ Q\n",
    "\n",
    "    # Householder transformation\n",
    "    def _get_bmm_householder_matrices(self):\n",
    "        N, S, _ = self.vectors.size()\n",
    "\n",
    "        outer = torch.bmm(self.vectors, self.vectors.transpose(1, 2))\n",
    "        inner = torch.bmm(self.vectors.transpose(1, 2), self.vectors)\n",
    "        I = torch.eye(S, device=self.vectors.device).expand(N, -1, -1)\n",
    "        hh_matrices = I - 2 * outer / (inner + 1e-16)\n",
    "        return hh_matrices\n",
    "\n",
    "    @staticmethod\n",
    "    def _reduce_mm(matrices):\n",
    "        Q = matrices[0]\n",
    "        for M in matrices[1:]:\n",
    "            Q = torch.mm(Q, M)\n",
    "        return Q\n",
    "\n",
    "\n",
    "    def bmm_naive_cascade(self):\n",
    "        \"\"\"\n",
    "        Args:   \n",
    "            vectors: [NumVectors, Size, 1]\n",
    "        Output:\n",
    "            Q: [Size, Size]\n",
    "        \"\"\"\n",
    "        matrices = self._get_bmm_householder_matrices()\n",
    "        return HH._reduce_mm(matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.prior = distributions.MultivariateNormal(torch.zeros(size, device=device), torch.eye(size, device=device))\n",
    "        self.encoder = nn.Sequential(CouplingLayer(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size),                                   \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer(size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        log_det_J = x.new_zeros(x.shape[0])\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, CouplingLayer):\n",
    "                x, log_det_J = layer((x, log_det_J))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x, log_det_J\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        for layer in reversed(self.encoder):\n",
    "            y = layer.inverse(y) \n",
    "        return y\n",
    "    \n",
    "    def sample(self, batchSize=1):\n",
    "        y = self.prior.sample((batchSize,))\n",
    "        return self.inverse(y)\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        y, logp = self(x)\n",
    "        return self.prior.log_prob(y) - logp\n",
    "    \n",
    "class Nice(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.prior = distributions.MultivariateNormal(torch.zeros(size, device=device), torch.eye(size, device=device))\n",
    "        self.encoder = nn.Sequential(\n",
    "                                     CouplingLayer_nice(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size),\n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size), \n",
    "                                     HH(size, 5),\n",
    "                                     CouplingLayer_nice(size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)   \n",
    "    \n",
    "    def inverse(self, y):\n",
    "        for layer in reversed(self.encoder):\n",
    "            y = layer.inverse(y) \n",
    "        return y\n",
    "    \n",
    "    def sample(self, batchSize=1):\n",
    "        y = self.prior.sample((batchSize,))\n",
    "        return self.inverse(y)\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        y = self(x)\n",
    "        return self.prior.log_prob(y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network):\n",
    "    net = network(IM_SIZE).to(device).train()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for X, _ in trainset:\n",
    "            X = X.view(-1, IM_SIZE).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = -net.log_prob(X).mean()            \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()  \n",
    "        print(epoch, loss)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2519.0557, device='cuda:0', grad_fn=<NegBackward>)\n",
      "1 tensor(1556.5509, device='cuda:0', grad_fn=<NegBackward>)\n",
      "2 tensor(1128.1848, device='cuda:0', grad_fn=<NegBackward>)\n",
      "3 tensor(1026.8461, device='cuda:0', grad_fn=<NegBackward>)\n",
      "4 tensor(955.1705, device='cuda:0', grad_fn=<NegBackward>)\n",
      "5 tensor(919.1803, device='cuda:0', grad_fn=<NegBackward>)\n",
      "6 tensor(924.9233, device='cuda:0', grad_fn=<NegBackward>)\n",
      "7 tensor(865.9039, device='cuda:0', grad_fn=<NegBackward>)\n",
      "8 tensor(840.2078, device='cuda:0', grad_fn=<NegBackward>)\n",
      "9 tensor(823.3857, device='cuda:0', grad_fn=<NegBackward>)\n",
      "10 tensor(818.9835, device='cuda:0', grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "net_RealNVP = train(RealNVP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net_RealNVP.eval()\n",
    "for X, _ in testset:\n",
    "    X = X[:10].view(-1, IM_SIZE).to(device)\n",
    "    y = net_RealNVP(X)\n",
    "    x = net_RealNVP.inverse(y)\n",
    "    break\n",
    "    \n",
    "for i, im in enumerate(x):  \n",
    "    plt.imshow(X[i].cpu().view(IM_SHAPE).detach().numpy())\n",
    "    plt.show()    \n",
    "    plt.imshow(im.cpu().view(IM_SHAPE).detach().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net):\n",
    "    net.eval()\n",
    "    comp_data = np.empty((10000, 784))\n",
    "    comp_label = np.empty((10000,))\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testset):\n",
    "            X = X.view(-1, IM_SIZE).to(device)\n",
    "            latent = net(X)\n",
    "            output = net.inverse(latent)\n",
    "            comp_data[BATCH * i: BATCH * (i + 1)] = latent.cpu()\n",
    "            comp_label[BATCH * i: BATCH * (i + 1)] = y\n",
    "\n",
    "            loss += F.mse_loss(output, X)\n",
    "\n",
    "    print(loss / (i + 1))\n",
    "    if use_cuda:\n",
    "        X = X.cpu()\n",
    "        output = output.cpu()\n",
    "        \n",
    "    plt.subplot(141)\n",
    "    plt.imshow(X[0].view(IM_SHAPE))\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(comp_data[0].reshape(IM_SHAPE))\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(output[0].view(IM_SHAPE))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comp_data, comp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(net_RealNVP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_RealNVP.eval()\n",
    "X = net_RealNVP.sample().cpu()\n",
    "plt.imshow(X[0].view(IM_SHAPE).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_RealNVP.eval()\n",
    "SAMPLES = 10\n",
    "X = net_RealNVP.sample(SAMPLES).cpu()\n",
    "for i in range(SAMPLES):\n",
    "    plt.imshow(X[i].view(IM_SHAPE).detach().numpy())\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
