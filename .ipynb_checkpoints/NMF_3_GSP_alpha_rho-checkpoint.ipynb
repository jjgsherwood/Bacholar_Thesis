{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import inspect\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from scipy import optimize, ndimage\n",
    "from sklearn import decomposition, cluster, model_selection, metrics\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils.dataset_utils as dataset\n",
    "import utils.train_utils as train\n",
    "\n",
    "import numpy.polynomial.polynomial as poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector_norm(X):\n",
    "    return (X.T / np.sqrt((X**2).sum(axis=1))).T\n",
    "\n",
    "def split_Raman_af(X):\n",
    "    \"\"\"\n",
    "    Removing spikes from the data to extract the autofluorescence.\n",
    "    This is done by applying smoothing filter to the data and then taking the min of the smoothing filter and original data.\n",
    "    \"\"\"\n",
    "    a = X\n",
    "    c = 50\n",
    "\n",
    "    # remove the top of the spikes from data, by using a Gaussian smoothing filter\n",
    "    for _ in range(5):      \n",
    "        a[:,c] = X[:,c]\n",
    "        a[:,-c] = X[:,-c]      \n",
    "        a1 = ndimage.gaussian_filter(a, (0, 30), mode='nearest')\n",
    "        a = np.min([a, a1], axis=0)\n",
    "\n",
    "    # remove the spikes from data, by using a polynominal fit\n",
    "    for _ in range(5):\n",
    "        a[:,c] = X[:,c]\n",
    "        a[:,-c] = X[:,-c]        \n",
    "        z = poly.polyfit(wavelength[::5], a[:,::5].T, 5)\n",
    "        a1 = poly.polyval(wavelength, z)\n",
    "        a = np.min([a, a1], axis=0)\n",
    "        \n",
    "    # smooth the curve the data, (to remove remnants of noise in the photoluminescence signal)\n",
    "    for _ in range(10):           \n",
    "        a[:,1] = X[:,1]\n",
    "        a[:,-1] = X[:,-1]         \n",
    "        a = ndimage.gaussian_filter(a, (0, 10), mode='nearest')\n",
    "\n",
    "    # make the Raman signal non-negative, (to remove remnants of noise in the Raman signal)\n",
    "    return (X-a).clip(min=0), a \n",
    "\n",
    "def smoothing(X, smooth=5, transition=10, spike_width=7):\n",
    "    \"\"\"\n",
    "    Only remove noise from low noise to signal area's to maintain the intensity of the spikes.\n",
    "    Noise is removed with a gaussian filter in spectral dimension.\n",
    "    \"\"\"\n",
    "    grad = ndimage.gaussian_filter(X, (0, 1), order=1)\n",
    "    grad_abs = np.abs(grad)\n",
    "    grad_abs_sm = ndimage.gaussian_filter(grad_abs, (0, 5))\n",
    "    mean_grad = np.mean(grad_abs, 1) + 1 / np.std(grad_abs, 1) * 3\n",
    "    \n",
    "    spikes = ((grad_abs_sm.T > mean_grad ).astype(float)).T \n",
    "    spikes = np.round(ndimage.gaussian_filter(spikes, (0, spike_width)))\n",
    "    spikes = ndimage.uniform_filter(spikes, (0, transition))\n",
    "    \n",
    "    return (1 - spikes) * ndimage.gaussian_filter(X, (0,smooth)) + spikes * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAVE = 2126\n",
    "\n",
    "X = np.load(\"../data/Raman/Alina_art_1_1.npy\", 'r')[:,:,2:N_WAVE+2]\n",
    "Y = np.load(\"../data/Raman/Alina_art_2_1.npy\", 'r')\n",
    "Z = np.load(\"../data/Raman/Alina_Art_4_2.npy\", 'r')\n",
    "\n",
    "wavelength = np.load(\"../data/Raman/wavelength.npy\", 'r')\n",
    "\n",
    "shape_X = X.shape \n",
    "shape_Y = Y.shape\n",
    "shape_Z = Z.shape \n",
    "\n",
    "X = copy.copy(X.reshape(-1, X.shape[-1]))\n",
    "X_smooth = smoothing(X)\n",
    "ram_X, afl_X = split_Raman_af(X_smooth)\n",
    "\n",
    "Y = copy.copy(Y.reshape(-1, Y.shape[-1]))\n",
    "Y_smooth = smoothing(Y)\n",
    "ram_Y, afl_Y = split_Raman_af(Y_smooth)\n",
    "\n",
    "Z = copy.copy(Z.reshape(-1, Z.shape[-1]))\n",
    "Z_smooth = smoothing(Z)\n",
    "ram_Z, afl_Z = split_Raman_af(Z_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferenceVectorClassifierNMF(BaseEstimator):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}        \n",
    "        self.set_params(**kwargs)\n",
    "        \n",
    "    def fit(self, x, **kwargs):\n",
    "        self.set_params(**kwargs)\n",
    "        X = unit_vector_norm(x)\n",
    "        \n",
    "        ###################### NNMF ################################        \n",
    "        nmf = NMF(self.kwargs['n_clusters'], alpha=0, l1_ratio=0.5)\n",
    "        W = nmf.fit_transform(X)\n",
    "        H = nmf.components_\n",
    "        self.nmf = nmf\n",
    "                \n",
    "        ###################### reference spectra ################################\n",
    "        self.reference_spectra_ = H\n",
    "        self.ref_org = unit_vector_norm(W.T @ x)\n",
    "        \n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict transforms the data into the reference space. Min weight should be 0 or higher then 'min_weight'\n",
    "        The error is the NMSE, where the MSE is normalised by the signal strength. \n",
    "        error.shape = X.shape[0], so for each data point the error is calculated.\n",
    "        \"\"\"\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        X = unit_vector_norm(X)\n",
    "        \n",
    "        ###################### RCA ################################\n",
    "        RCA_vector = self.nmf.transform(X)\n",
    "    \n",
    "        return RCA_vector  \n",
    "    \n",
    "    def get_reference_vectors(self):\n",
    "        return self.reference_spectra_\n",
    "    \n",
    "    def get_org_reference_vectors(self):\n",
    "        return self.ref_org\n",
    "\n",
    "    def get_params(self, deep=False):\n",
    "        return self.kwargs\n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        self.kwargs.update(kwargs)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_map(estimator, X, y=None):\n",
    "    RCA = estimator.predict(X)\n",
    "    ref_vec = estimator.get_reference_vectors()\n",
    "    return ((RCA @ ref_vec - X)**2).sum(1)\n",
    "\n",
    "def score_func(estimator, X, y=None):\n",
    "    tmp = np.sqrt((X**2).sum(axis=1))\n",
    "    X = unit_vector_norm(X)\n",
    "    return (error_map(estimator, X) * tmp).mean()\n",
    "\n",
    "def print_mean_std(X):\n",
    "    return f\"{X.mean():<12.4e}{X.std():<12.4e}\"\n",
    "\n",
    "def cross_val_X_Y_Z(rvc, X, Y, Z):\n",
    "    rvc.fit(np.concatenate((X, Y), axis=0))\n",
    "    return score_func(rvc, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'n_clusters': 3,\n",
    "          'n_components': 3,\n",
    "          'batch_size': 16,\n",
    "          'cuda': True,\n",
    "          'log_step': 10,\n",
    "          'epochs': 10,\n",
    "          'depth': 5,\n",
    "          'neurons': 500,\n",
    "          'bias': True\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    rvc = ReferenceVectorClassifierNMF(**kwargs)\n",
    "    rvc.fit(ram_X)\n",
    "    score.append(score_func(rvc, ram_X))\n",
    "print(score)\n",
    "print(np.mean(score), np.std(score))\n",
    "del score[np.argmax(score)]\n",
    "del score[np.argmin(score)]\n",
    "print(score)\n",
    "print(np.mean(score), np.std(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    rvc = ReferenceVectorClassifierNMF(**kwargs)\n",
    "    rvc.fit(ram_Y)\n",
    "    score.append(score_func(rvc, ram_Y))\n",
    "print(score)\n",
    "print(np.mean(score), np.std(score))\n",
    "del score[np.argmax(score)]\n",
    "del score[np.argmin(score)]\n",
    "print(score)\n",
    "print(np.mean(score), np.std(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    rvc = ReferenceVectorClassifierNMF(**kwargs)\n",
    "    rvc.fit(ram_Z)\n",
    "    score.append(score_func(rvc, ram_Z))\n",
    "print(score)\n",
    "print(np.mean(score), np.std(score))\n",
    "del score[np.argmax(score)]\n",
    "del score[np.argmin(score)]\n",
    "print(score)\n",
    "print(np.mean(score), np.std(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
