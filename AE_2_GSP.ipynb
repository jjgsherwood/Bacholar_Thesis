{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import inspect\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from scipy import optimize, ndimage\n",
    "from sklearn import decomposition, cluster, model_selection, metrics\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils.dataset_utils as dataset\n",
    "import utils.train_utils as train\n",
    "\n",
    "import numpy.polynomial.polynomial as poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector_norm(X):\n",
    "    return (X.T / np.sqrt((X**2).sum(axis=1))).T\n",
    "\n",
    "def split_Raman_af(X):\n",
    "    \"\"\"\n",
    "    Removing spikes from the data to extract the autofluorescence.\n",
    "    This is done by applying smoothing filter to the data and then taking the min of the smoothing filter and original data.\n",
    "    \"\"\"\n",
    "    a = X\n",
    "    c = 50\n",
    "\n",
    "    # remove the top of the spikes from data, by using a Gaussian smoothing filter\n",
    "    for _ in range(5):      \n",
    "        a[:,c] = X[:,c]\n",
    "        a[:,-c] = X[:,-c]      \n",
    "        a1 = ndimage.gaussian_filter(a, (0, 30), mode='nearest')\n",
    "        a = np.min([a, a1], axis=0)\n",
    "\n",
    "    # remove the spikes from data, by using a polynominal fit\n",
    "    for _ in range(5):\n",
    "        a[:,c] = X[:,c]\n",
    "        a[:,-c] = X[:,-c]        \n",
    "        z = poly.polyfit(wavelength[::5], a[:,::5].T, 5)\n",
    "        a1 = poly.polyval(wavelength, z)\n",
    "        a = np.min([a, a1], axis=0)\n",
    "        \n",
    "    # smooth the curve the data, (to remove remnants of noise in the photoluminescence signal)\n",
    "    for _ in range(10):           \n",
    "        a[:,1] = X[:,1]\n",
    "        a[:,-1] = X[:,-1]         \n",
    "        a = ndimage.gaussian_filter(a, (0, 10), mode='nearest')\n",
    "\n",
    "    # make the Raman signal non-negative, (to remove remnants of noise in the Raman signal)\n",
    "    return (X-a).clip(min=0), a \n",
    "\n",
    "def smoothing(X, smooth=5, transition=10, spike_width=7):\n",
    "    \"\"\"\n",
    "    Only remove noise from low noise to signal area's to maintain the intensity of the spikes.\n",
    "    Noise is removed with a gaussian filter in spectral dimension.\n",
    "    \"\"\"\n",
    "    grad = ndimage.gaussian_filter(X, (0, 1), order=1)\n",
    "    grad_abs = np.abs(grad)\n",
    "    grad_abs_sm = ndimage.gaussian_filter(grad_abs, (0, 5))\n",
    "    mean_grad = np.mean(grad_abs, 1) + 1 / np.std(grad_abs, 1) * 3\n",
    "    \n",
    "    spikes = ((grad_abs_sm.T > mean_grad ).astype(float)).T \n",
    "    spikes = np.round(ndimage.gaussian_filter(spikes, (0, spike_width)))\n",
    "    spikes = ndimage.uniform_filter(spikes, (0, transition))\n",
    "    \n",
    "    return (1 - spikes) * ndimage.gaussian_filter(X, (0,smooth)) + spikes * X\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAVE = 2126\n",
    "\n",
    "# X = np.load(\"../data/Raman/Alina_art_1_1.npy\", 'r')\n",
    "Y = np.load(\"../data/Raman/Alina_Art_4_2.npy\", 'r')\n",
    "\n",
    "wavelength = np.load(\"../data/Raman/wavelength.npy\", 'r')\n",
    "\n",
    "# shape_X = X.shape \n",
    "shape_Y = Y.shape\n",
    "\n",
    "# X = copy.copy(X.reshape(-1, X.shape[-1]))\n",
    "Y = copy.copy(Y.reshape(-1, Y.shape[-1]))\n",
    "\n",
    "# ram_X, afl_X = split_Raman_af(X)\n",
    "# ram_smooth_X = smoothing(ram_X, s)\n",
    "\n",
    "Y = smoothing(Y, 5, 2)\n",
    "ram_smooth_Y, afl_Y = split_Raman_af(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n_components=10, depth=2, neurons=100, bias=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encode = nn.Sequential( \n",
    "            nn.Dropout3d(0.25),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(N_WAVE, neurons, bias=bias),\n",
    "            nn.ReLU(True),\n",
    "            *((nn.Linear(neurons, neurons, bias=bias),\n",
    "            nn.ReLU(True)) * (depth-1)),\n",
    "            nn.Linear(neurons, n_components, bias=bias),\n",
    "            nn.Softmax(1),\n",
    "        )\n",
    "        \n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(n_components, neurons, bias=bias),\n",
    "            nn.ReLU(True),\n",
    "            *((nn.Linear(neurons, neurons, bias=bias),\n",
    "            nn.ReLU(True)) * (depth-1)),            \n",
    "            nn.Linear(neurons, N_WAVE, bias=bias),\n",
    "            View((-1,1,1,1,N_WAVE)),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_loss(x, model):\n",
    "    W = model.encode(x)\n",
    "    x_ = model.decode(W)\n",
    "    \n",
    "    method = 3\n",
    "\n",
    "    # maximize the difference in reference vectors\n",
    "    if method == 0:\n",
    "        reference_spectra_ = W.T @ x.squeeze()\n",
    "        max_ref_diff = 0\n",
    "        for i in range(W.size(1)):\n",
    "            for j in range(i + 1, W.size(1)):\n",
    "                max_ref_diff += 1 / (torch.abs(reference_spectra_[i] - reference_spectra_[j]).sum() + 1)\n",
    "    elif method == 1:\n",
    "        max_ref_diff = 0\n",
    "        for i in range(W.size(1)):\n",
    "            for j in range(i + 1, W.size(1)):\n",
    "                max_ref_diff += 1 / (torch.abs(W[i] - W[j]).sum() + 1)\n",
    "    elif method == 2:\n",
    "        max_ref_diff = torch.abs(W).sum(1).mean(0)\n",
    "    elif method == 3:\n",
    "        max_ref_diff = torch.abs(W).sum(1).mean(0)        \n",
    "        max_ref_diff += ((W.sum(1))**2).mean(0)\n",
    "    else:\n",
    "        max_ref_diff = 0\n",
    "        \n",
    "    #MSE loss\n",
    "    MSE = ((x_ - x)**2).sum(4).sum()  \n",
    "    \n",
    "    return MSE + max_ref_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferenceVectorClassifierAE(BaseEstimator):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}\n",
    "        self.ae_kwargs = {}        \n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "        _use_cuda = torch.cuda.is_available() and kwargs['cuda']\n",
    "        if _use_cuda:\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.device = torch.device('cuda' if _use_cuda else 'cpu')        \n",
    "        \n",
    "    def fit(self, x, **kwargs):\n",
    "        self.set_params(**kwargs)\n",
    "        X = unit_vector_norm(x)\n",
    "        \n",
    "        ###################### Autoencoder ################################\n",
    "        self.model = AutoEncoder(**self.ae_kwargs).to(self.device)\n",
    "        \n",
    "        parameters = filter(lambda x: x.requires_grad, self.model.parameters())\n",
    "        self.optimizer = optim.Adam(parameters)        \n",
    "        train_loader, test_loader = dataset.load_liver(X, self.kwargs['batch_size'])\n",
    "        \n",
    "        for epoch in range(self.kwargs['epochs']):\n",
    "            print('-'*50)\n",
    "            print('Epoch {:3d}/{:3d}'.format(epoch+1, self.kwargs['epochs']))\n",
    "            start_time = datetime.now()\n",
    "            train.train(self.model, self.optimizer, train_loader, self.kwargs['loss_func'], self.kwargs['log_step'], self.device)\n",
    "            end_time = datetime.now()\n",
    "            time_diff = relativedelta(end_time, start_time)\n",
    "            print('Elapsed time: {}h {}m {}s'.format(time_diff.hours, time_diff.minutes, time_diff.seconds))\n",
    "            loss = train.test(self.model, test_loader, self.kwargs['loss_func'], self.device)\n",
    "            print('Validation| bits: {:2.2f}'.format(loss), flush=True)    \n",
    "          \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            W = self.model.encode(dataset.load_liver_all(X).to(self.device))\n",
    "        self.z = W\n",
    "        W = W.cpu().detach().numpy()\n",
    "                          \n",
    "        ###################### reference spectra ################################\n",
    "        self.reference_spectra_ = unit_vector_norm(W.T @ X)    \n",
    "        self.ref_org = unit_vector_norm(W.T @ x)\n",
    "                \n",
    "        # Return the classifier\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict transforms the data into the reference space. Min weight should be 0 or higher then 'min_weight'\n",
    "        The error is the NMSE, where the MSE is normalised by the signal strength. \n",
    "        error.shape = X.shape[0], so for each data point the error is calculated.\n",
    "        \"\"\"\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        X = unit_vector_norm(X)\n",
    "        \n",
    "        ###################### RCA ################################           \n",
    "        RCA_vector = np.array([optimize.nnls(self.reference_spectra_.T, X[i,:])[0] for i in range(X.shape[0])])\n",
    "\n",
    "        return RCA_vector\n",
    "    \n",
    "    def get_reference_vectors(self):\n",
    "        return self.reference_spectra_\n",
    "\n",
    "    def get_org_reference_vectors(self):\n",
    "        return self.ref_org    \n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return self.kwargs\n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        self.kwargs.update(kwargs)\n",
    "        self.ae_kwargs.update({k:v  for k,v in kwargs.items() if k in list(inspect.signature(AutoEncoder).parameters.keys())})     \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_map(estimator, X, y=None):\n",
    "    RCA = estimator.predict(X)\n",
    "    ref_vec = estimator.get_reference_vectors()\n",
    "    return ((RCA @ ref_vec - X)**2).sum(1)\n",
    "\n",
    "def score_func(estimator, X, y=None):\n",
    "    X = unit_vector_norm(X)\n",
    "    return error_map(estimator, X).mean()\n",
    "\n",
    "def print_mean_std(X):\n",
    "    return f\"{X.mean():<12.4e}{X.std():<12.4e}\"\n",
    "\n",
    "def cross_val_X_Y_Z(rvc, X, Y, Z):\n",
    "    rvc.fit(np.concatenate((X, Y), axis=0))\n",
    "    return score_func(rvc, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'n_components': 6,\n",
    "          'batch_size': 32,\n",
    "          'cuda': True,\n",
    "          'log_step': 30,\n",
    "          'loss_func': MSE_loss,\n",
    "          'epochs': 5,\n",
    "          'depth': 3,\n",
    "          'neurons': N_WAVE,\n",
    "          'bias': True\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvc = ReferenceVectorClassifierAE(**kwargs)\n",
    "rvc.fit(ram_smooth_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCA_vector = rvc.predict(ram_smooth_Y)\n",
    "\n",
    "print(\"fit score: \", score_func(rvc, ram_smooth_Y))\n",
    "\n",
    "RCA_vector = RCA_vector - RCA_vector.min(0)\n",
    "RCA_vector /= RCA_vector.max(0)\n",
    "\n",
    "plt.figure(figsize = (20,6))\n",
    "plt.imshow(RCA_vector.reshape((*shape_Y[:2], kwargs['n_components']))[::-1,:,:3])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "plt.figure(figsize = (20,6))\n",
    "plt.imshow(RCA_vector.reshape((*shape_Y[:2], kwargs['n_components']))[::-1,:,3:6])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "plt.figure(figsize = (20,6))\n",
    "plt.imshow(error_map(rvc, ram_smooth_Y).reshape(shape_Y[:2]), cmap='gray', vmin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most of the image (background) is 'red' (when comparing to the ref_vector)\n",
    "\n",
    "the sport in the lower right corner is hard to identify could be white.\n",
    "\n",
    "probably blue is not visible in the raman data.\n",
    "\n",
    "green is the mayor part of the \"pinquin\"\n",
    "\n",
    "The edge between the \"pinquin\" and background is a mixure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6), dpi=500)\n",
    "for i, r in enumerate(rvc.get_reference_vectors()):\n",
    "    plt.plot(wavelength[:N_WAVE], r)\n",
    "# for i, r in enumerate(rvc.get_reference_vectors()):  \n",
    "#     if i in [2,3, 4]:\n",
    "#         if i == 2:\n",
    "#             plt.plot(wavelength[:N_WAVE], r, 'b', label='auto_encoder_heatmap_1_blue')\n",
    "#         elif i == 4:\n",
    "#             plt.plot(wavelength[:N_WAVE], r, 'g', label='auto_encoder_heatmap_2_green')\n",
    "#         else:\n",
    "#             plt.plot(wavelength[:N_WAVE], r, 'r', label='auto_encoder_heatmap_2_red')\n",
    "#     if i == 1:\n",
    "#         plt.plot(wavelength[:N_WAVE], r, 'g', label='auto_encoder_heatmap_1_green')\n",
    "#     if i in [0,5]:\n",
    "#         if i == 0:\n",
    "#             plt.plot(wavelength[:N_WAVE], r, 'r', label='auto_encoder_heatmap_1_red')\n",
    "#         else:\n",
    "#             plt.plot(wavelength[:N_WAVE], r, 'b', label='auto_encoder_heatmap_2_blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructing plot\n",
    "with torch.no_grad():\n",
    "    x_ = rvc.model(dataset.load_liver_all(unit_vector_norm(ram_smooth_Y)).to('cuda:0'))\n",
    "\n",
    "plt.figure(figsize = (20,12))\n",
    "\n",
    "for i in range(0,667,1):\n",
    "    plt.plot(wavelength[:N_WAVE], x_[i].flatten().cpu().detach().numpy(),label='rec')\n",
    "#     plt.plot(wavelength, unit_vector_norm(ram_smooth)[i] , label='org')    \n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W = rvc.model.encode(dataset.load_liver_all(unit_vector_norm(ram_smooth_Y)).to('cuda:0'))\n",
    "        \n",
    "W = W.cpu().detach().numpy()\n",
    "W1 = W - W.min(0)\n",
    "W1 /= W1.max(0)\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.imshow(W1.reshape((*shape_Y[:2], kwargs['n_components']))[::-1,:,:3])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.imshow(W1.reshape((*shape_Y[:2], kwargs['n_components']))[::-1,:,3:6])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
